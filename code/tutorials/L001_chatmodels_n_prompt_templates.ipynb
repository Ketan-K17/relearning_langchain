{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# url: https://python.langchain.com/docs/tutorials/llm_chain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "# model.invoke(messages)\n",
    "model.invoke(messages).content\n",
    "\n",
    "# Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles (e.g. human, assistant, system) and hold important data, such as tool calls and token usage counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\n",
    "\n",
    "print(model.invoke(\"Hello\").content)\n",
    "\n",
    "\n",
    "print(model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}]).content) # this is the openai format (URL: https://python.langchain.com/docs/concepts/messages/#openai-format)\n",
    "\n",
    "print(model.invoke([HumanMessage(\"Hello\")]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STREAMING\n",
    "\n",
    "# Since chat models are runnables, we can stream tokens out of it too, since most runnables have a stream method. We can also call them async.\n",
    "\n",
    "messages = [HumanMessage(\"Write me a paragraph about Joan of Arc.\")]\n",
    "\n",
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT TEMPLATES: https://python.langchain.com/docs/concepts/prompt_templates/\n",
    "\n",
    "# You'll often need to construct prompts dynamically, for example, to fill in a template with a user's name, or something like that. You can use the various prompt templates by langchain to do this. For chat messages, you can use the ChatPromptTemplate.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
    "\n",
    "# prompt\n",
    "# print(prompt)\n",
    "# print(prompt.to_string())\n",
    "print(prompt.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
